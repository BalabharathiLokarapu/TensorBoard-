{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## What-If Tool from scratch - From CSV to trained binary classification model to What-If Tool usage\n",
    "\n",
    "# This notebook shows the process of loading up a dataset from CSV, training a very simple classifier to\n",
    "# predict one of the columns, then using the What-If Tool (WIT) to analyze the training dataset and the trained\n",
    "# model.\n",
    "\n",
    "# This notebook uses the UCI Census dataset and learning problem, detailed at\n",
    "# https://archive.ics.uci.edu/ml/datasets/census+income, which predicts whether a person earns more than $50k\n",
    "# given their census information.\n",
    "# To customize this notebook to work on your own dataset, you only need to edit the sections marked with \"USER: \"\n",
    "\n",
    "## Setup (install Jupyter, Tensorflow, and Tensorflow Serving in a virtualenv).\n",
    "# NOTE: Use of a virtualenv, pip installation of tensorflow and docker use for TF Serving aren't the only way\n",
    "# to set all this up. I just find it the simplest and safest to use.\n",
    "\n",
    "# Step 1: Install Tensorflow using pip/virtualenv - See https://www.tensorflow.org/install/pip for instructions\n",
    "\n",
    "# Step 2: Install Tensorflow Serving using docker - See https://www.tensorflow.org/serving/docker for instructions\n",
    "\n",
    "# The next steps must be done from a terminal that has activated the virtualenv that was created in step 1\n",
    "\n",
    "# Step 3: Install Jupyter and pandas to view and run this notebook\n",
    "# > pip install jupyter\n",
    "# > pip install pandas\n",
    "\n",
    "# Step 4: Run this notebook\n",
    "# > jupyter notebook\n",
    "# From the file selector that opens in the browser, select this notebook file.\n",
    "# Run the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load helper functions\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import data\n",
    "\n",
    "# Writes a pandas dataframe to disk as a tfrecord file of tf.Example protos,\n",
    "# using only the dataframe columns specified. Non-numeric columns are treated\n",
    "# as strings.\n",
    "def write_df_as_tfrecord(df, filename, columns=None):\n",
    "    if not os.path.exists(os.path.dirname(filename)):\n",
    "        os.makedirs(os.path.dirname(filename))\n",
    "    writer = tf.python_io.TFRecordWriter(filename)\n",
    "    if columns == None:\n",
    "        columns = df.columns.values.tolist()\n",
    "    for index, row in df.iterrows():\n",
    "        example = tf.train.Example()\n",
    "        for col in columns:\n",
    "            if df[col].dtype is np.dtype(np.int64):\n",
    "                example.features.feature[col].int64_list.value.append(row[col])\n",
    "            elif df[col].dtype is np.dtype(np.float64):\n",
    "                example.features.feature[col].float_list.value.append(row[col])\n",
    "            elif row[col] == row[col]:\n",
    "                example.features.feature[col].bytes_list.value.append(row[col].encode('utf-8'))\n",
    "        writer.write(example.SerializeToString())\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "# Creates a tf feature spec from the dataframe and columns specified.\n",
    "def create_feature_spec(df, columns):\n",
    "    feature_spec = {}\n",
    "    for f in columns:\n",
    "        if df[f].dtype is np.dtype(np.int64):\n",
    "            feature_spec[f] = tf.FixedLenFeature(shape=(), dtype=tf.int64)\n",
    "        elif df[f].dtype is np.dtype(np.float64):\n",
    "            feature_spec[f] = tf.FixedLenFeature(shape=(), dtype=tf.float32)\n",
    "        else:\n",
    "            feature_spec[f] = tf.FixedLenFeature(shape=(), dtype=tf.string)\n",
    "    return feature_spec\n",
    "\n",
    "# Parses a serialized tf.Example into input features and target feature from \n",
    "# the provided label feature name and feature spec.\n",
    "def parse_tf_example(example_proto, label, feature_spec):\n",
    "    parsed_features = tf.parse_example(serialized=example_proto, features=feature_spec)\n",
    "    target = parsed_features.pop(label)\n",
    "    return parsed_features, target\n",
    "\n",
    "# An input function for providing input to a model from tf.Examples from tf record files.\n",
    "def tfrecords_input_fn(files_name_pattern, feature_spec, label, mode=tf.estimator.ModeKeys.EVAL,\n",
    "                       num_epochs=None, \n",
    "                       batch_size=64):\n",
    "    shuffle = True if mode == tf.estimator.ModeKeys.TRAIN else False\n",
    "    file_names = tf.matching_files(files_name_pattern)\n",
    "    dataset = data.TFRecordDataset(filenames=file_names)\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=2 * batch_size + 1)\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(lambda tf_example: parse_tf_example(tf_example, label, feature_spec))\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    features, target = iterator.get_next()\n",
    "    return features, target\n",
    "\n",
    "# Creates simple numeric and categorical feature columns from a feature spec and a\n",
    "# list of columns from that spec to use.\n",
    "#\n",
    "# NOTE: Models might perform better with some feature engineering such as bucketed\n",
    "# numeric columns and hash-bucket/embedding columns for categorical features.\n",
    "def create_feature_columns(columns, feature_spec):\n",
    "    ret = []\n",
    "    for col in columns:\n",
    "        if feature_spec[col].dtype is tf.int64 or feature_spec[col].dtype is tf.float32:\n",
    "            ret.append(tf.feature_column.numeric_column(col))\n",
    "        else:\n",
    "            ret.append(tf.feature_column.indicator_column(\n",
    "                tf.feature_column.categorical_column_with_vocabulary_list(col, list(df[col].unique()))))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the dataset from a CSV into dataframe and display a list of all columns and a preview of the data\n",
    "\n",
    "# USER: Set the path to the CSV containing the dataset to train on (can be a web address or local path).\n",
    "csv_path = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
    "\n",
    "# USER: Set the column names for the columns in the CSV. If the CSV's first line is a header line containing\n",
    "# the column names, then set this to None.\n",
    "csv_columns = [\"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Marital-Status\",\n",
    "               \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital-Gain\", \"Capital-Loss\",\n",
    "               \"Hours-per-week\", \"Country\", \"Target\"]\n",
    "\n",
    "# Read the dataset from the provided CSV and print out information about it.\n",
    "df = pd.read_csv(csv_path, names=csv_columns, skipinitialspace=True)\n",
    "print df.columns.tolist()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER: Set the name you want to give the directory the model will be saved to\n",
    "model_name = 'trained_model'\n",
    "\n",
    "# USER: Set the name you want to give the tfrecord dataset file\n",
    "tfrecord_name = 'data.tfrecord'\n",
    "\n",
    "# USER: Set the column in the dataset you wish for the model to predict\n",
    "label_column = 'Target'\n",
    "\n",
    "model_path = os.path.join(os.getcwd(), model_name)\n",
    "tfrecord_path = os.path.join(os.getcwd(), tfrecord_name)\n",
    "\n",
    "# USER: Make the label column numeric (0 and 1), for use in our model.\n",
    "# In this case, examples with a target value of '<=50K' are considered to be in the '0' (negative) class\n",
    "# and all other examples are considered to be in the '1' (positive) class.\n",
    "df[label_column] = np.where(df[label_column] == '<=50K', 0, 1)\n",
    "\n",
    "# USER: If the CSV needs any clean-up (such as removing problematic rows or creating new columns), do it here.\n",
    "\n",
    "# USER: Set list of all columns from the dataset we will use for model input.\n",
    "input_features = ['Age', 'Workclass', 'Education', 'Marital-Status', 'Occupation', 'Relationship', 'Race', 'Sex',\n",
    "                  'Capital-Gain', 'Capital-Loss', 'Hours-per-week', 'Country']\n",
    "\n",
    "# Ensure the label column is not accidentally set as an input feature.\n",
    "if label_column in input_features:\n",
    "    input_features.remove(label_column)\n",
    "\n",
    "# Create a list containing all input features and the label column\n",
    "features_and_labels = input_features + [label_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the records to disk as tf.Example protos in tf record file, for use in model training\n",
    "# and later for use by WIT.\n",
    "write_df_as_tfrecord(df, tfrecord_path, features_and_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create and train the classifier\n",
    "\n",
    "import functools\n",
    "\n",
    "# Create a feature spec for the classifier\n",
    "feature_spec = create_feature_spec(df, features_and_labels)\n",
    "\n",
    "# Define and train the classifier\n",
    "train_inpf = functools.partial(tfrecords_input_fn, tfrecord_path, feature_spec, label_column)\n",
    "classifier = tf.estimator.LinearClassifier(\n",
    "    feature_columns=create_feature_columns(input_features, feature_spec))\n",
    "classifier.train(train_inpf, steps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the classifier to disk for serving\n",
    "\n",
    "# Uses a parsing serving input receiver function so that it can classify from serialized tf.Examples\n",
    "# using the TensorFlow Serving Classify API.\n",
    "\n",
    "serving_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)\n",
    "classifier.export_savedmodel(model_path, serving_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print out the What-If Tool usage instructions (serve model, launch TensorBoard, configure What-If Tool)\n",
    "import urllib\n",
    "\n",
    "docker_command = 'docker run -p 8500:8500 --mount type=bind,source=%s,target=/models/my_model/ -e MODEL_NAME=my_model -t tensorflow/serving' % model_path\n",
    "what_if_tool_path = ('http://localhost:6006/#whatif&inferenceAddress=%s&modelName=my_model&examplesPath=%s' % \n",
    "                     (urllib.quote('localhost:8500'), urllib.quote(tfrecord_path)))\n",
    "\n",
    "print 'Command to serve model (depending on your docker installation, you may need to add \"sudo\" to the front of the docker command):'\n",
    "print docker_command\n",
    "print '\\n'\n",
    "\n",
    "\n",
    "print 'Command to launch tensorboard:'\n",
    "print 'tensorboard --logdir .'\n",
    "print '\\n'\n",
    "\n",
    "\n",
    "print 'URL to view What-If Tool for your model and dataset:'\n",
    "print what_if_tool_path\n",
    "\n",
    "# To kill the served model, find the docker container ID through 'docker container ls',\n",
    "# then run 'docker kill [containerId]' (run as sudo as necessary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
